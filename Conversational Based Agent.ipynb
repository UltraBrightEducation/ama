{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKV2BEL76euo"
   },
   "source": [
    "<h1 style=\"text-align: center;text-transform: uppercase;\">Conversational Based Agent</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "In this project, you will build an end-to-end voice conversational agent, which can take a voice input audio line, and synthesize a response. The chatbot agent will be executed locally on your computer. \n",
    "\n",
    "<img style=\"width:550px; height:300px;\" src=\"assets/intro.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9fuw6EH4LOU"
   },
   "source": [
    "This jupyter notebook is consists of the following parts:\n",
    "1. __Speech Recognition:__ <br>In this part, you will create a speech recognition that can convert your voice into a text format.<br><br>\n",
    "2. __Chatbot:__ <br>This is the core of your conversational based agent. You will build a chatbot that will answer your questions. <br><br>\n",
    "3. __Text to Speech:__ <br>After getting the answer from your chatbot, it should be converted into a voice format and that is what you should create in this part. <br><br>\n",
    "4. __Finalize your Conversational Based Agent:__ <br>At the very end step, you will put everything together and create your Conversational Based Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gx13lGaR6evT"
   },
   "source": [
    "<br>\n",
    "\n",
    "# 1. Speech Recognition\n",
    "\n",
    "---\n",
    "\n",
    "We will use the Mozilla <a href=\"https://github.com/mozilla/DeepSpeech\">DeepSpeech</a> open-sourced implementation originally developed by Baidu. This allows speech recognition directly on your computer instead of requiring an internet connection or setting up a cloud account.\n",
    "\n",
    "While DeepSpeech is not the state-of-the-art speech recognizer (there is now DeepSpeech2, Wave2Letter by Facebook, and \n",
    "the RNN Transducer by Google), DeepSpeech is a fast, lightweight implementation which is suitable for real-time transcription with very high accuracy. Its code is also well-maintained with new features being added regularly.\n",
    "\n",
    "\n",
    "In this project, we will not train our own speech recognition model (a fairly challenging project), but will use an open-sourced pre-trained model.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepspeech\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "from scipy.io.wavfile import write\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import queue\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = deepspeech.Model('speech_recognizer/deepspeech-0.7.4-models.pbmm')\n",
    "ds.enableExternalScorer('speech_recognizer/deepspeech-0.7.4-models.scorer')\n",
    "_ = ds.setScorerAlphaBeta(0.75, 1.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Speech-recognition on single audio file\n",
    "\n",
    "In this section, let's set up the basic functionality of running speech recognition on a single audio file. \n",
    "\n",
    "1. recording a .wav audio file with a fix d length (say 3 seconds)\n",
    "\n",
    "2. perform speech recognition from the saved .wav file using the DeepSpeech model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name = 'audio_files/test_audio.wav'\n",
    "sample_rate = 16000\n",
    "seconds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording Finished!\n"
     ]
    }
   ],
   "source": [
    "sleep(0.5)\n",
    "print(\"Recording...\")\n",
    "audio_array = sd.rec(int(seconds * sample_rate), samplerate = sample_rate, channels = 1)\n",
    "\n",
    "# Wait until recording is finished\n",
    "sd.wait() \n",
    "\n",
    "# Finished recording print\n",
    "print(\"Recording Finished!\")\n",
    "\n",
    "# Save as WAV file \n",
    "write(test_file_name, sample_rate, audio_array) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sd.rec` function gives us numpy array directly! We can check its shape\n",
    "\n",
    "The number of rows is seconds * sample_rate = 16000 * 4, the number of columns is the channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 1)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the recording by playing it back from the numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0721478e-02],\n",
       "       [ 8.8054694e-02],\n",
       "       [-2.7549866e-01],\n",
       "       ...,\n",
       "       [ 8.7510690e-04],\n",
       "       [ 1.4510446e-41],\n",
       "       [ 1.7442986e+28]], dtype=float32)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd.playrec(audio_array, sample_rate, channels=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or from the .wav file that it is saved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, fs = sf.read(test_file_name, dtype='float32')\n",
    "sd.play(data, sample_rate, device=1)\n",
    "status = sd.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the playback did not work, chooose another output device by checking what is available on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "> 0 Built-in Microphone, Core Audio (2 in, 0 out)\n",
       "< 1 Built-in Output, Core Audio (0 in, 2 out)\n",
       "  2 USB PnP Audio Device, Core Audio (0 in, 2 out)\n",
       "  3 USB PnP Audio Device, Core Audio (1 in, 0 out)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd.query_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While sound device outputs numpy array in float32 datatype (from -1 to 1), DeepSpeech speech recognizer expects a 16bit int type (-32768 to 32767). Let's convert the numpy array and set the correct data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_array *= 32768\n",
    "audio_array = audio_array.astype('int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test recording'"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.stt(audio_array[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Streaming Speech Recognition in Real-Time\n",
    "\n",
    "Recording your voice then running speech recognition on a audio file works fine, but it is not very user friendly. The interaction is slow and not easy to use in a continuous setting.\n",
    "\n",
    "In this section, let's setup a function to recording your voice AND recognize the text at the same time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(indata, frames, time, status):\n",
    "    \"\"\"This is called (from a separate thread) for each audio block.\"\"\"\n",
    "    if status:\n",
    "        print(status, file=sys.stderr)\n",
    "    q.put(indata.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "press Ctrl+C to stop the recording\n",
      "################################################################################\n",
      "\n",
      "the moon is about two hundred and fifty thousand miles from earth on average\n"
     ]
    }
   ],
   "source": [
    "q = queue.Queue()\n",
    "recognizer_stream = ds.createStream()\n",
    "try:\n",
    "    with sd.InputStream(samplerate=sample_rate, device=0, channels=2, callback=callback) as audio_stream:\n",
    "        print('#' * 80)\n",
    "        print('press Interrupt to stop the recording')\n",
    "        print('#' * 80)\n",
    "        print()\n",
    "        i = 0\n",
    "        while True:\n",
    "            i += 1\n",
    "            audio_chunk = q.get()\n",
    "            audio_chunk *= 32768\n",
    "            audio_chunk = audio_chunk.astype('int16')\n",
    "            recognizer_stream.feedAudioContent(audio_chunk[:,0])\n",
    "            text = recognizer_stream.intermediateDecode()\n",
    "            print(f'\\r{text}', end='')\n",
    "except KeyboardInterrupt:\n",
    "#     print('\\r\\nRecording finished.\\r\\n')\n",
    "    pass\n",
    "finally:\n",
    "    audio_stream.stop()\n",
    "    audio_stream.close()\n",
    "    audio_chunks = []\n",
    "    while True:\n",
    "        if not q.empty():\n",
    "            chunk = q.get()\n",
    "            audio_chunks.append(chunk)\n",
    "        else:\n",
    "            break\n",
    "    if audio_chunks:\n",
    "        audio_chunks = np.concatenate(audio_chunks)\n",
    "        audio_chunk *= 32768\n",
    "        audio_chunk = audio_chunk.astype('int16')\n",
    "        recognizer_stream.feedAudioContent(audio_chunk[:,0])\n",
    "    text = recognizer_stream.finishStream()\n",
    "    print(f'\\r{text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_recognition():\n",
    "    q = queue.Queue()\n",
    "    recognizer_stream = ds.createStream()\n",
    "    \n",
    "    def callback(indata, frames, time, status):\n",
    "        \"\"\"This is called (from a separate thread) for each audio block.\"\"\"\n",
    "        if status:\n",
    "            print(status, file=sys.stderr)\n",
    "        q.put(indata.copy())\n",
    "    \n",
    "    try:\n",
    "        with sd.InputStream(samplerate=sample_rate, device=0, channels=2, callback=callback) as audio_stream:\n",
    "            while True:\n",
    "                audio_chunk = q.get()\n",
    "                audio_chunk *= 32768\n",
    "                audio_chunk = audio_chunk.astype('int16')\n",
    "                recognizer_stream.feedAudioContent(audio_chunk[:,0])\n",
    "                text = recognizer_stream.intermediateDecode()\n",
    "                print(f\"\\r - YOU SAID: {text}\", end='')\n",
    "    except KeyboardInterrupt:\n",
    "    #     print('\\r\\nRecording finished.\\r\\n')\n",
    "        pass\n",
    "    finally:\n",
    "        audio_stream.stop()\n",
    "        audio_stream.close()\n",
    "        audio_chunks = []\n",
    "        while True:\n",
    "            if not q.empty():\n",
    "                chunk = q.get()\n",
    "                audio_chunks.append(chunk)\n",
    "            else:\n",
    "                break\n",
    "        if audio_chunks:\n",
    "            audio_chunks = np.concatenate(audio_chunks)\n",
    "            audio_chunk *= 32768\n",
    "            audio_chunk = audio_chunk.astype('int16')\n",
    "            recognizer_stream.feedAudioContent(audio_chunk[:,0])\n",
    "        text = recognizer_stream.finishStream()\n",
    "        print(f\"\\r - YOU SAID: {text}\", end='\\r\\n')\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "press Interrupt to stop the recording\n",
      "################################################################################\n",
      "\n",
      "this is a test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'this is a test'"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming_recognition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congratulations! You are now able to run your own speech-to-text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tIqinjUX6ewC"
   },
   "source": [
    "<br>\n",
    "\n",
    "# 2. Chatbot\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In this part, you will create a deep learning based conversational agent. This agent will be able to interact with users and understand their questions. More specifically, you will start with loading the dataset, cleaning and preprocessing them, and then you will feed them into a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iJi6wO7v4LOw"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.1. Load and Clean the Chatterbot Dataset \n",
    "\n",
    "---\n",
    "\n",
    "In this project, we have provided you with multiple dataset files. Each of these files contains conversations regarding a specific topic. For example, topics about humor, food, movies, science, history, etc. You can read the description of each dataset in below:\n",
    "\n",
    "| Name of Dataset | Description |\n",
    "| :----:| :----: |\n",
    "| botprofile.yml | Personality of Your Chatbot |\n",
    "| humor.yml | Joke and Humor |\n",
    "| emotion.yml | Emotional Conversations |\n",
    "| politics.yml | Political Conversations |\n",
    "| ai.yml | General Questions about AI |\n",
    "| computers.yml | Conversations about Computer |\n",
    "| history.yml | Q&A about Historical Facts and Events |\n",
    "| psychology.yml | Psychological Conversations |\n",
    "| food.yml | Food Related Conversations. |\n",
    "| literature.yml | Conversations about Different Books, Authors, Genres |\n",
    "| money.yml | Conversations about Money, Investment, Economy |\n",
    "| trivia.yml | Conversations that Have Small Values |\n",
    "| gossip.yml | Gossipy Conversations |\n",
    "| conversations.yml | Common Conversations |\n",
    "| greetings.yml | Different Ways of Greeting |\n",
    "| sports.yml | Conversations about Sports. |\n",
    "| movies.yml | Conversation about Movies. |\n",
    "| science.yml | Conversations about Science  |\n",
    "| health.yml | Health Related Questions and Answers. |\n",
    "\n",
    "\n",
    "Feel free to modify these datasets in the way you want the chatbot to behave. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xKZLku6c4LOx"
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import yaml\n",
    "from yaml import Loader\n",
    "import glob\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UYYGIupd4LO0"
   },
   "outputs": [],
   "source": [
    "# Function for loading all of the yml files\n",
    "def load_chatterbot_dataset():\n",
    "    \n",
    "    # Initialize empty lists for questions and answers\n",
    "    questions, answers = [], []\n",
    "    \n",
    "    # Get the list of all dataset names\n",
    "    dataset_names = glob.glob(\"datasets/chatterbot/*.yml\")\n",
    "    \n",
    "    # Iterate through each dataset name\n",
    "    for i_dataset_name in tqdm(dataset_names):\n",
    "        \n",
    "        # Load the dataset\n",
    "        with open(i_dataset_name) as file:\n",
    "            greeting = yaml.load(file, Loader = Loader)[\"conversations\"]\n",
    "            \n",
    "        # Iterate through each conversation\n",
    "        for i_conversation in greeting:\n",
    "            \n",
    "            # If length is two\n",
    "            if len(i_conversation) == 2:\n",
    "                \n",
    "                # Append the question to 'questions' list\n",
    "                questions.append(i_conversation[0])\n",
    "                \n",
    "                # Append the answer to 'answers' list\n",
    "                answers.append(i_conversation[1])\n",
    "            \n",
    "            # If length is more than two\n",
    "            elif len(i_conversation) > 2:\n",
    "                \n",
    "                # Iterate through each index\n",
    "                for index in range(len(i_conversation)-1):\n",
    "    \n",
    "                    # Append the question and answer\n",
    "                    questions.append(i_conversation[0])\n",
    "                    answers.append(i_conversation[index+1])\n",
    "                    \n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7693,
     "status": "ok",
     "timestamp": 1575580064620,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "iI3-jw9w4LO9",
    "outputId": "2ba271ef-f620-43f7-b56a-6c40af19fb72"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 89.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get the questions and answers\n",
    "questions, answers = load_chatterbot_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Question & Answers:  869\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Question & Answers: \", len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 0: \n",
      " Good morning, how are you?\n",
      "\n",
      "Answer 0: \n",
      " I'm also good.\n",
      "--------------------------------------------------------------------------\n",
      "Question 1: \n",
      " What makes you sad\n",
      "\n",
      "Answer 1: \n",
      " Sadness is not an emotion that I like to experience.\n",
      "--------------------------------------------------------------------------\n",
      "Question 2: \n",
      " Tell me a joke\n",
      "\n",
      "Answer 2: \n",
      " what do you get when you cross a dance and a cheetah?\n",
      "--------------------------------------------------------------------------\n",
      "Question 3: \n",
      " you are emotional\n",
      "\n",
      "Answer 3: \n",
      " i certainly do at times.\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the preprocessed questions and answers\n",
    "total_questions = len(questions)\n",
    "for i in range(4):\n",
    "    j = random.randint(0, total_questions)\n",
    "    print(\"Question {}: \\n\".format(i), questions[j])\n",
    "    print(\"\")\n",
    "    print(\"Answer {}: \\n\".format(i), answers[j])\n",
    "    print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPDgm3I66exF"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.4. Data Preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "After cleaning the dataset, you should preprocess the dataset by following the below steps:\n",
    "\n",
    "1. Lower case the text.\n",
    "2. Decontract the text (e.g. she's -> she is, they're -> they are, etc.).\n",
    "3. Remove the punctuation (e.g. !, ?, $, %, #, @, ^, etc.).\n",
    "4. Tokenization.\n",
    "5. Pad the sequences to be the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6642,
     "status": "ok",
     "timestamp": 1575580089447,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "5qJaF_hI6exG",
    "outputId": "84a518e3-adc1-4da7-a7eb-ac73f4259008"
   },
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import numpy as np\n",
    "import contractions\n",
    "import re\n",
    "from tensorflow.keras import preprocessing, utils\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L6ihwP5f6exI"
   },
   "outputs": [],
   "source": [
    "# Function for preprocessing the given text\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Decontracting the text (e.g. it's -> it is)\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Remove the punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the questions and answers\n",
    "questions = [preprocess_text(q) for q in questions]\n",
    "answers = [preprocess_text(q) for q in answers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1575580095488,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "hQ5J_WQ36exP",
    "outputId": "0aac5118-815d-4a81-f7e7-7876f5f84941"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 0: \n",
      " what is the stock market\n",
      "\n",
      "Answer 0: \n",
      " trading shares \n",
      "--------------------------------------------------------------------------\n",
      "Question 1: \n",
      " how much do you earn\n",
      "\n",
      "Answer 1: \n",
      " i am expecting a raise soon \n",
      "--------------------------------------------------------------------------\n",
      "Question 2: \n",
      " chemistry\n",
      "\n",
      "Answer 2: \n",
      " my favorite subject is chemistry\n",
      "--------------------------------------------------------------------------\n",
      "Question 3: \n",
      " robots are not allowed to lie\n",
      "\n",
      "Answer 3: \n",
      " sure we are   we choose not to \n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the preprocessed questions and answers\n",
    "total_questions = len(questions)\n",
    "for i in range(4):\n",
    "    j = random.randint(0, total_questions)\n",
    "    print(\"Question {}: \\n\".format(i), questions[j])\n",
    "    print(\"\")\n",
    "    print(\"Answer {}: \\n\".format(i), answers[j])\n",
    "    print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that every training example are the type string, we need to first filter out both answers and questions that are not string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rgu9h3Ga4LPU"
   },
   "outputs": [],
   "source": [
    "# answers_with_tags = list()\n",
    "# for i in range(len(answers)):\n",
    "#     if type(answers[i]) == str:\n",
    "#         answers_with_tags.append(answers[i])\n",
    "#     else:\n",
    "#         questions.pop(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPRw_ueq6ex-"
   },
   "source": [
    "After preprocessing the dataset, we should add a start tag (e.g. `<START>`) and an end tag (e.g. `<END>`) to answers. Remember that we will only add these tags to answers and not questions. This requirement is because of the Seq2Seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xan8xpTC6eyC"
   },
   "outputs": [],
   "source": [
    "# Add <START> and <END> tag to each sentence\n",
    "answers = ['starttoken ' + a + ' endtoken' for a in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1575580101816,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "T-gutAKp6eyD",
    "outputId": "3f620206-f516-4865-eb03-f9ba206b47bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starttoken i am capable of interacting with my environment and reacting to events in it  which is the essence of experience   therefore  your statement is incorrect  endtoken\n",
      "starttoken a computer is an electronic device which takes information in digital form and performs a series of operations based on predetermined instructions to give some output  endtoken\n",
      "starttoken what do you want to know  endtoken\n",
      "starttoken i certainly do not last as long as i would want to  endtoken\n",
      "starttoken complex is better than complicated  endtoken\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(random.choice(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XxbjI3zBU-b1"
   },
   "source": [
    "Now it's time to tokenize our dataset. We use a class in Keras which allows us to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 793,
     "status": "ok",
     "timestamp": 1575580108328,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "p9FnHJsA6eyF",
    "outputId": "7d3d127b-309d-46bf-ea62-d4956b13b7af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 1939\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "\n",
    "# Fit the tokenizer to questions and answers\n",
    "tokenizer.fit_on_texts(questions + answers)\n",
    "\n",
    "# Get the total vocab size\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "\n",
    "print( 'VOCAB SIZE : {}'.format(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 540,
     "status": "ok",
     "timestamp": 1575580109511,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "sC6cafRA6eyI",
    "outputId": "bc954abd-b859-4eb9-cb4b-9639b3c0fca7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(869, 22) 22\n"
     ]
    }
   ],
   "source": [
    "### encoder input data\n",
    "\n",
    "# Tokenize the questions\n",
    "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
    "\n",
    "# Get the length of longest sequence\n",
    "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
    "\n",
    "# Pad the sequences\n",
    "padded_questions = pad_sequences(tokenized_questions, maxlen=maxlen_questions, padding='post')\n",
    "\n",
    "# Convert the sequences into array\n",
    "encoder_input_data = np.array(padded_questions)\n",
    "\n",
    "print(encoder_input_data.shape, maxlen_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1575580110574,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "1urkx83k6eyJ",
    "outputId": "5edc1e24-8113-4327-935e-3bcb07982b8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(869, 45) 45\n"
     ]
    }
   ],
   "source": [
    "### decoder input data\n",
    "\n",
    "# Tokenize the answers\n",
    "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
    "\n",
    "# Get the length of longest sequence\n",
    "maxlen_answers = max([len(x) for x in tokenized_answers])\n",
    "\n",
    "# Pad the sequences\n",
    "padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
    "\n",
    "# Convert the sequences into array\n",
    "decoder_input_data = np.array(padded_answers)\n",
    "\n",
    "print(decoder_input_data.shape, maxlen_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 994,
     "status": "ok",
     "timestamp": 1575580112277,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "hvq32nEI6eyL",
    "outputId": "14eb24de-e193-4151-d3d2-6a947e432621"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(869, 45, 1939)\n"
     ]
    }
   ],
   "source": [
    "### decoder_output_data\n",
    "\n",
    "# Iterate through index of tokenized answers\n",
    "for i in range(len(tokenized_answers)):\n",
    "\n",
    "    #\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "\n",
    "# Pad the tokenized answers\n",
    "padded_answers = pad_sequences(tokenized_answers, maxlen = maxlen_answers, padding = 'post')\n",
    "\n",
    "# One hot encode\n",
    "onehot_answers = utils.to_categorical(padded_answers, VOCAB_SIZE)\n",
    "\n",
    "# Convert to numpy array\n",
    "decoder_output_data = np.array(onehot_answers)\n",
    "\n",
    "print(decoder_output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W7Cz8WVO4LP9"
   },
   "outputs": [],
   "source": [
    "# Saving all the arrays to storage\n",
    "np.save(\"enc_in_data.npy\", encoder_input_data)\n",
    "np.save(\"dec_in_data.npy\", decoder_input_data)\n",
    "np.save(\"dec_tar_data.npy\", decoder_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ClM9JxbyeJ0C"
   },
   "outputs": [],
   "source": [
    "# Load all the arrays from storage\n",
    "encoder_input_data = np.load(\"enc_in_data.npy\")\n",
    "decoder_input_data = np.load(\"dec_in_data.npy\")\n",
    "decoder_output_data = np.load(\"dec_tar_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PCTc1O_meJ0F"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.5. Train the Seq2Seq Model\n",
    "\n",
    "---\n",
    "\n",
    "In this section, we will use an architecture called Sequence to Sequence (or Seq2Seq). This model is used since the length of the input sequence (question) does not match the length of the output sequence (answer). This model is consists of an encoder and a decoder.\n",
    "- __Encoder:__ <br> In this part of the network, we take the input data and train on it. Then we pass the last state of the recurrent layer to decoder. <br><br>\n",
    "- __Decoder:__ <br> In this part of the network, we take the last state in encoder’s last recurrent layer. Then we will use it as an initial state in decoder's first recurrent layer.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"assets/encoder_decoder.png\">\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's start by importing all the necessary libraries in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w521SVsleJ0F"
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eq9ZLFSEKmNi"
   },
   "source": [
    "Below you can play around with hyperparameters for improving the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hvhvarggeJ0P"
   },
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RL28XUCmLb93"
   },
   "source": [
    "In the following block of code, you will implement the Encoder. You can follow the below steps for creating the encoder: \n",
    "\n",
    "1.   Create an input for the Encoder.\n",
    "2.   Create an embedding layer.\n",
    "3.   Create an LSTM layer which also returns the states.\n",
    "4.   Get the hidden state (state h) and cell state (state c) inside a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlV38YEgeJ0Q"
   },
   "outputs": [],
   "source": [
    "### Encoder Input\n",
    "embed_dim = 200\n",
    "num_lstm = 200\n",
    "\n",
    "# Input for encoder\n",
    "encoder_inputs = Input(shape = (None, ), name='encoder_inputs')\n",
    "\n",
    "# Embedding layer\n",
    "# Why mask_zero = True? https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "encoder_embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = embed_dim, mask_zero = True, name='encoder_embedding')(encoder_inputs)\n",
    "\n",
    "# LSTM layer (that returns states in addition to output)\n",
    "encoder_outputs, state_h, state_c = LSTM(units = num_lstm, return_state = True, name='encoder_lstm')(encoder_embedding)\n",
    "\n",
    "# Get the states for encoder\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svSUWa7NOlKq"
   },
   "source": [
    "After creating your encoder, it's time to implement the decoder. You can follow the below steps for implementing the decoder:\n",
    "\n",
    "1.   Create an input for the decoder.\n",
    "2.   Create an embedding layer.\n",
    "3.   Create an LSTM layer that returns states and sequences.\n",
    "4.   Create a dense layer.\n",
    "5.   Get the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-UaTlPLDeJ0R"
   },
   "outputs": [],
   "source": [
    "### Decoder\n",
    "\n",
    "# Input for decoder\n",
    "decoder_inputs = Input(shape = (None,  ), name='decoder_inputs')\n",
    "\n",
    "# Embedding layer\n",
    "decoder_embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = embed_dim , mask_zero = True, name='decoder_embedding')(decoder_inputs)\n",
    "\n",
    "# LSTM layer (that returns states and sequences as well)\n",
    "decoder_lstm = LSTM(units = num_lstm , return_state = True , return_sequences = True, name='decoder_lstm')\n",
    "\n",
    "# Get the output of LSTM layer, using the initial states from the encoder\n",
    "decoder_outputs, _, _ = decoder_lstm(inputs = decoder_embedding, initial_state = encoder_states)\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense = Dense(units = VOCAB_SIZE, activation = softmax, name='output') \n",
    "\n",
    "# Get the output of Dense layer\n",
    "output = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QaOFGnnoQynS"
   },
   "source": [
    "Now that you have implemented the encoder and decoder. It's time to create your model which takes two inputs: encoder's input and decoder's input. Then it outputs the decoder's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8k0ErcMZeJ0T"
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Model([encoder_inputs, decoder_inputs], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VamQBht8eJ0U"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer = RMSprop(lr = LEARNING_RATE), loss = \"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1575581175908,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "CKrCrmA26eyQ",
    "outputId": "49f944e7-a3ae-4e33-9622-755d28fd8b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, None, 200)    387800      encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, None, 200)    387800      decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, 200), (None, 320800      encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 200),  320800      decoder_embedding[0][0]          \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, None, 1939)   389739      decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,806,939\n",
      "Trainable params: 1,806,939\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 869 samples\n",
      "Epoch 1/100\n",
      "869/869 [==============================] - 9s 11ms/sample - loss: 1.4653\n",
      "Epoch 2/100\n",
      "869/869 [==============================] - 8s 9ms/sample - loss: 1.2848\n",
      "Epoch 3/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 1.2320\n",
      "Epoch 4/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 1.1936\n",
      "Epoch 5/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 1.1620\n",
      "Epoch 6/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 1.1305\n",
      "Epoch 7/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 1.1005\n",
      "Epoch 8/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 1.0700\n",
      "Epoch 9/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 1.0410\n",
      "Epoch 10/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 1.0121\n",
      "Epoch 11/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.9857\n",
      "Epoch 12/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.9601\n",
      "Epoch 13/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.9364\n",
      "Epoch 14/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.9137\n",
      "Epoch 15/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.8918\n",
      "Epoch 16/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.8714\n",
      "Epoch 17/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.8513\n",
      "Epoch 18/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.8317\n",
      "Epoch 19/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.8128\n",
      "Epoch 20/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.7925\n",
      "Epoch 21/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.7739\n",
      "Epoch 22/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.7548\n",
      "Epoch 23/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.7360\n",
      "Epoch 24/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.7182\n",
      "Epoch 25/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.7002\n",
      "Epoch 26/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.6820\n",
      "Epoch 27/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.6645\n",
      "Epoch 28/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.6478\n",
      "Epoch 29/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.6305\n",
      "Epoch 30/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.6138\n",
      "Epoch 31/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.5975\n",
      "Epoch 32/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.5819\n",
      "Epoch 33/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.5653\n",
      "Epoch 34/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.5492\n",
      "Epoch 35/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.5331\n",
      "Epoch 36/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.5181\n",
      "Epoch 37/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.5037\n",
      "Epoch 38/100\n",
      "869/869 [==============================] - 8s 9ms/sample - loss: 0.4879\n",
      "Epoch 39/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.4736\n",
      "Epoch 40/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.4602\n",
      "Epoch 41/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.4453\n",
      "Epoch 42/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.4314\n",
      "Epoch 43/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.4188\n",
      "Epoch 44/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.4060\n",
      "Epoch 45/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.3925\n",
      "Epoch 46/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.3802\n",
      "Epoch 47/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.3683\n",
      "Epoch 48/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.3566\n",
      "Epoch 49/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.3449\n",
      "Epoch 50/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.3330\n",
      "Epoch 51/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.3224\n",
      "Epoch 52/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.3118\n",
      "Epoch 53/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.3010\n",
      "Epoch 54/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.2920\n",
      "Epoch 55/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.2808\n",
      "Epoch 56/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.2723\n",
      "Epoch 57/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.2624\n",
      "Epoch 58/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.2537\n",
      "Epoch 59/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.2450\n",
      "Epoch 60/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.2362\n",
      "Epoch 61/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.2280\n",
      "Epoch 62/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.2212\n",
      "Epoch 63/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.2116\n",
      "Epoch 64/100\n",
      "869/869 [==============================] - 8s 9ms/sample - loss: 0.2046\n",
      "Epoch 65/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.1981\n",
      "Epoch 66/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.1897\n",
      "Epoch 67/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.1841\n",
      "Epoch 68/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.1766\n",
      "Epoch 69/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.1702\n",
      "Epoch 70/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.1650\n",
      "Epoch 71/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.1582\n",
      "Epoch 72/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.1530\n",
      "Epoch 73/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.1475\n",
      "Epoch 74/100\n",
      "869/869 [==============================] - 8s 9ms/sample - loss: 0.1417\n",
      "Epoch 75/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.1365\n",
      "Epoch 76/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.1310\n",
      "Epoch 77/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.1263\n",
      "Epoch 78/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.1223\n",
      "Epoch 79/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.1174\n",
      "Epoch 80/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.1134\n",
      "Epoch 81/100\n",
      "869/869 [==============================] - 8s 9ms/sample - loss: 0.1087\n",
      "Epoch 82/100\n",
      "869/869 [==============================] - 9s 10ms/sample - loss: 0.1054\n",
      "Epoch 83/100\n",
      "869/869 [==============================] - 8s 9ms/sample - loss: 0.1011\n",
      "Epoch 84/100\n",
      "869/869 [==============================] - 8s 9ms/sample - loss: 0.0980\n",
      "Epoch 85/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.0943\n",
      "Epoch 86/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.0914\n",
      "Epoch 87/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.0882\n",
      "Epoch 88/100\n",
      "869/869 [==============================] - 8s 10ms/sample - loss: 0.0852\n",
      "Epoch 89/100\n",
      "869/869 [==============================] - 8s 9ms/sample - loss: 0.0824\n",
      "Epoch 90/100\n",
      "869/869 [==============================] - 8s 9ms/sample - loss: 0.0800\n",
      "Epoch 91/100\n",
      "869/869 [==============================] - 8s 10ms/sample - loss: 0.0774\n",
      "Epoch 92/100\n",
      "869/869 [==============================] - 8s 9ms/sample - loss: 0.0744\n",
      "Epoch 93/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.0725\n",
      "Epoch 94/100\n",
      "869/869 [==============================] - 8s 9ms/sample - loss: 0.0701\n",
      "Epoch 95/100\n",
      "869/869 [==============================] - 9s 10ms/sample - loss: 0.0687\n",
      "Epoch 96/100\n",
      "869/869 [==============================] - 8s 9ms/sample - loss: 0.0663\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.0646\n",
      "Epoch 98/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.0632\n",
      "Epoch 99/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.0619\n",
      "Epoch 100/100\n",
      "869/869 [==============================] - 7s 8ms/sample - loss: 0.0600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x186393fd0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x = [encoder_input_data , decoder_input_data], \n",
    "          y = decoder_output_data, \n",
    "          batch_size = BATCH_SIZE, \n",
    "          epochs = EPOCHS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2870,
     "status": "ok",
     "timestamp": 1575581735556,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "9ERetSNY4LRH",
    "outputId": "d52d02a7-54ed-478e-e0f3-604e8f534884"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Weight Saved!\n"
     ]
    }
   ],
   "source": [
    "# Save the final model\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "model.save(filepath = f'saved_models/final_weight_{timestamp}.h5') \n",
    "print(f\"Model Weight Saved to {final_weight_{timestamp}.h5}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer that needs to be used in conjunction with the sequence modelso we can use it elsewhere\n",
    "with open(f'saved_models/tokenizer_{timestamp}.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1385,
     "status": "ok",
     "timestamp": 1575581737849,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "tkVPvGd74LRK",
    "outputId": "5a3a9d6e-4684-417d-f7ae-8824b57f0a8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Weight Loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load the final model\n",
    "model.load_weights(f'saved_models/final_weight_2020-08-01-16-28-47.h5') \n",
    "print(\"Model Weight Loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F33sverZeJ0b"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.6. Inference\n",
    "\n",
    "---\n",
    "\n",
    "Now it's time to use our model for inference. In other words, we will ask a question to our chatbot and it will answer us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GaDAPDOa6eyb"
   },
   "outputs": [],
   "source": [
    "# Function for making inference\n",
    "def make_inference_models():\n",
    "    \n",
    "    # Create a model that takes encoder's input and outputs the states for encoder\n",
    "    encoder_model = tensorflow.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    # Create two inputs for decoder which are hidden state (or state h) and cell state (or state c)\n",
    "    decoder_state_input_h = Input(shape = (num_lstm, ))\n",
    "    decoder_state_input_c = Input(shape = (num_lstm, ))\n",
    "    \n",
    "    # Store the two inputs for decoder inside a list\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    # Pass the inputs through LSTM layer you have created before\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state = decoder_states_inputs)\n",
    "    \n",
    "    # Store the outputted hidden state and cell state from LSTM inside a list\n",
    "    decoder_states = [state_h, state_c]\n",
    "\n",
    "    # Pass the output from LSTM layer through the dense layer you have created before\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Create a model that takes decoder_inputs and decoder_states_inputs as inputs and outputs decoder_outputs and decoder_states\n",
    "    decoder_model = tensorflow.keras.models.Model([decoder_inputs] + decoder_states_inputs,\n",
    "                          [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NWYFiofO6eye"
   },
   "outputs": [],
   "source": [
    "# Function for converting strings to tokens\n",
    "def str_to_tokens(sentence:str):\n",
    "\n",
    "    # Lowercase the sentence and split it into words\n",
    "    words = sentence.lower().split()\n",
    "\n",
    "    # Initialize a list for tokens\n",
    "    tokens_list = list()\n",
    "\n",
    "    # Iterate through words\n",
    "    for word in words:\n",
    "\n",
    "        # Append the word index inside tokens list\n",
    "        tokens_list.append(tokenizer.word_index[word]) \n",
    "\n",
    "    # Pad the sequences to be the same length\n",
    "    return pad_sequences([tokens_list] , maxlen = maxlen_questions, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model for inference\n",
    "enc_model , dec_model = make_inference_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending conversational agent\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the number of times you want to ask question\n",
    "try:\n",
    "    for _ in range(5):\n",
    "\n",
    "        # Get the input and predict it with the encoder model\n",
    "        states_values = enc_model.predict(str_to_tokens(preprocess_text(input('Enter question : '))))\n",
    "\n",
    "        # Initialize the target sequence with zero - array([[0.]])\n",
    "        empty_target_seq = np.zeros(shape = (1, 1))\n",
    "\n",
    "        # Update the target sequence with index of \"start\"\n",
    "        empty_target_seq[0, 0] = tokenizer.word_index[\"starttoken\"]\n",
    "\n",
    "        # Initialize the stop condition with False\n",
    "        stop_condition = False\n",
    "\n",
    "        # Initialize the decoded words with an empty string\n",
    "        decoded_translation = []\n",
    "\n",
    "        # While stop_condition is false\n",
    "        while not stop_condition :\n",
    "\n",
    "            # Predict the (target sequence + the output from encoder model) with decoder model\n",
    "            dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values)\n",
    "\n",
    "            # Get the index for sampled word\n",
    "            sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
    "\n",
    "            # Initialize the sampled word with None\n",
    "            sampled_word = None\n",
    "\n",
    "            # Iterate through words and their indexes\n",
    "            for word, index in tokenizer.word_index.items() :\n",
    "\n",
    "                # If the index is equal to sampled word's index\n",
    "                if sampled_word_index == index :\n",
    "\n",
    "                    # Add the word to the decoded string\n",
    "                    decoded_translation.append(word)\n",
    "\n",
    "                    # Update the sampled word\n",
    "                    sampled_word = word\n",
    "\n",
    "            # If sampled word is equal to \"end\" OR the length of decoded string is more that what is allowed\n",
    "            if sampled_word == 'endtoken' or len(decoded_translation) > maxlen_answers:\n",
    "\n",
    "                # Make the stop_condition to true\n",
    "                stop_condition = True\n",
    "\n",
    "            # Initialize back the target sequence to zero - array([[0.]])    \n",
    "            empty_target_seq = np.zeros(shape = (1, 1))  \n",
    "\n",
    "            # Update the target sequence with index of \"start\"\n",
    "            empty_target_seq[0, 0] = sampled_word_index\n",
    "\n",
    "            # Get the state values\n",
    "            states_values = [h, c] \n",
    "\n",
    "            # Print the decoded string\n",
    "        print(' '.join(decoded_translation[:-1]))\n",
    "except KeyboardInterrupt:\n",
    "    print('Ending conversational agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k-z2MXSd6ezL"
   },
   "source": [
    "<br>\n",
    "\n",
    "# 4. Text to Speech\n",
    "\n",
    "---\n",
    "\n",
    "In this section, we will use a library called pyttsx3 which performs text-to-speech conversion. Unlike alternative libraries, this works offline and is compatible with both Python 2 and 3.\n",
    "\n",
    "State-of-the-art text-to-voice systems are more difficult to setup and use (such as the Tacotron 2 + WaveNet), and is outside the scope of this course. Interested students can look them up!\n",
    "\n",
    "<a href=\"https://ai.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html\">Tacotron 2</a> <br>\n",
    "https://arxiv.org/abs/1712.05884\n",
    "\n",
    "Checkout some demos using WaveNet.\n",
    "https://cloud.google.com/text-to-speech\n",
    "\n",
    "For a _relatively simple_ implementation of a high quality TTS system that you can run, checkout the Mozilla <a href=\"https://github.com/mozilla/TTS\">TTS</a> project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UOU6Y0DK6ezM"
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQ-pfu4t4LR7"
   },
   "outputs": [],
   "source": [
    "# Construct a new TTS engine instance\n",
    "engine = pyttsx3.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVD7Nu4o4LSE",
    "outputId": "fa4f114c-ea72-45a8-a254-6d9e39bc9fb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voice 0: \n",
      " - ID: com.apple.speech.synthesis.voice.Alex\n",
      " - Name: Alex\n",
      " - Languages: ['en_US']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 1: \n",
      " - ID: com.apple.speech.synthesis.voice.alice\n",
      " - Name: Alice\n",
      " - Languages: ['it_IT']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 2: \n",
      " - ID: com.apple.speech.synthesis.voice.alva\n",
      " - Name: Alva\n",
      " - Languages: ['sv_SE']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 3: \n",
      " - ID: com.apple.speech.synthesis.voice.amelie\n",
      " - Name: Amelie\n",
      " - Languages: ['fr_CA']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 4: \n",
      " - ID: com.apple.speech.synthesis.voice.anna\n",
      " - Name: Anna\n",
      " - Languages: ['de_DE']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 5: \n",
      " - ID: com.apple.speech.synthesis.voice.carmit\n",
      " - Name: Carmit\n",
      " - Languages: ['he_IL']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 6: \n",
      " - ID: com.apple.speech.synthesis.voice.damayanti\n",
      " - Name: Damayanti\n",
      " - Languages: ['id_ID']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 7: \n",
      " - ID: com.apple.speech.synthesis.voice.daniel\n",
      " - Name: Daniel\n",
      " - Languages: ['en_GB']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 8: \n",
      " - ID: com.apple.speech.synthesis.voice.diego\n",
      " - Name: Diego\n",
      " - Languages: ['es_AR']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 9: \n",
      " - ID: com.apple.speech.synthesis.voice.ellen\n",
      " - Name: Ellen\n",
      " - Languages: ['nl_BE']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 10: \n",
      " - ID: com.apple.speech.synthesis.voice.fiona\n",
      " - Name: Fiona\n",
      " - Languages: ['en-scotland']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 11: \n",
      " - ID: com.apple.speech.synthesis.voice.Fred\n",
      " - Name: Fred\n",
      " - Languages: ['en_US']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 30\n",
      "\n",
      "Voice 12: \n",
      " - ID: com.apple.speech.synthesis.voice.ioana\n",
      " - Name: Ioana\n",
      " - Languages: ['ro_RO']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 13: \n",
      " - ID: com.apple.speech.synthesis.voice.joana\n",
      " - Name: Joana\n",
      " - Languages: ['pt_PT']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 14: \n",
      " - ID: com.apple.speech.synthesis.voice.jorge\n",
      " - Name: Jorge\n",
      " - Languages: ['es_ES']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 15: \n",
      " - ID: com.apple.speech.synthesis.voice.juan\n",
      " - Name: Juan\n",
      " - Languages: ['es_MX']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 16: \n",
      " - ID: com.apple.speech.synthesis.voice.kanya\n",
      " - Name: Kanya\n",
      " - Languages: ['th_TH']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 17: \n",
      " - ID: com.apple.speech.synthesis.voice.karen\n",
      " - Name: Karen\n",
      " - Languages: ['en_AU']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 18: \n",
      " - ID: com.apple.speech.synthesis.voice.kyoko\n",
      " - Name: Kyoko\n",
      " - Languages: ['ja_JP']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 19: \n",
      " - ID: com.apple.speech.synthesis.voice.laura\n",
      " - Name: Laura\n",
      " - Languages: ['sk_SK']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 20: \n",
      " - ID: com.apple.speech.synthesis.voice.lekha\n",
      " - Name: Lekha\n",
      " - Languages: ['hi_IN']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 21: \n",
      " - ID: com.apple.speech.synthesis.voice.luca\n",
      " - Name: Luca\n",
      " - Languages: ['it_IT']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 22: \n",
      " - ID: com.apple.speech.synthesis.voice.luciana\n",
      " - Name: Luciana\n",
      " - Languages: ['pt_BR']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 23: \n",
      " - ID: com.apple.speech.synthesis.voice.maged\n",
      " - Name: Maged\n",
      " - Languages: ['ar_SA']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 24: \n",
      " - ID: com.apple.speech.synthesis.voice.mariska\n",
      " - Name: Mariska\n",
      " - Languages: ['hu_HU']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 25: \n",
      " - ID: com.apple.speech.synthesis.voice.mei-jia\n",
      " - Name: Mei-Jia\n",
      " - Languages: ['zh_TW']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 26: \n",
      " - ID: com.apple.speech.synthesis.voice.melina\n",
      " - Name: Melina\n",
      " - Languages: ['el_GR']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 27: \n",
      " - ID: com.apple.speech.synthesis.voice.milena\n",
      " - Name: Milena\n",
      " - Languages: ['ru_RU']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 28: \n",
      " - ID: com.apple.speech.synthesis.voice.moira\n",
      " - Name: Moira\n",
      " - Languages: ['en_IE']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 29: \n",
      " - ID: com.apple.speech.synthesis.voice.monica\n",
      " - Name: Monica\n",
      " - Languages: ['es_ES']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 30: \n",
      " - ID: com.apple.speech.synthesis.voice.nora\n",
      " - Name: Nora\n",
      " - Languages: ['nb_NO']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 31: \n",
      " - ID: com.apple.speech.synthesis.voice.paulina\n",
      " - Name: Paulina\n",
      " - Languages: ['es_MX']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 32: \n",
      " - ID: com.apple.speech.synthesis.voice.samantha\n",
      " - Name: Samantha\n",
      " - Languages: ['en_US']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 33: \n",
      " - ID: com.apple.speech.synthesis.voice.sara\n",
      " - Name: Sara\n",
      " - Languages: ['da_DK']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 34: \n",
      " - ID: com.apple.speech.synthesis.voice.satu\n",
      " - Name: Satu\n",
      " - Languages: ['fi_FI']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 35: \n",
      " - ID: com.apple.speech.synthesis.voice.sin-ji\n",
      " - Name: Sin-ji\n",
      " - Languages: ['zh_HK']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 36: \n",
      " - ID: com.apple.speech.synthesis.voice.tessa\n",
      " - Name: Tessa\n",
      " - Languages: ['en_ZA']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 37: \n",
      " - ID: com.apple.speech.synthesis.voice.thomas\n",
      " - Name: Thomas\n",
      " - Languages: ['fr_FR']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 38: \n",
      " - ID: com.apple.speech.synthesis.voice.ting-ting\n",
      " - Name: Ting-Ting\n",
      " - Languages: ['zh_CN']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 39: \n",
      " - ID: com.apple.speech.synthesis.voice.veena\n",
      " - Name: Veena\n",
      " - Languages: ['en_IN']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 40: \n",
      " - ID: com.apple.speech.synthesis.voice.Victoria\n",
      " - Name: Victoria\n",
      " - Languages: ['en_US']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 41: \n",
      " - ID: com.apple.speech.synthesis.voice.xander\n",
      " - Name: Xander\n",
      " - Languages: ['nl_NL']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 42: \n",
      " - ID: com.apple.speech.synthesis.voice.yelda\n",
      " - Name: Yelda\n",
      " - Languages: ['tr_TR']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 43: \n",
      " - ID: com.apple.speech.synthesis.voice.yuna\n",
      " - Name: Yuna\n",
      " - Languages: ['ko_KR']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 44: \n",
      " - ID: com.apple.speech.synthesis.voice.yuri\n",
      " - Name: Yuri\n",
      " - Languages: ['ru_RU']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 45: \n",
      " - ID: com.apple.speech.synthesis.voice.zosia\n",
      " - Name: Zosia\n",
      " - Languages: ['pl_PL']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 46: \n",
      " - ID: com.apple.speech.synthesis.voice.zuzana\n",
      " - Name: Zuzana\n",
      " - Languages: ['cs_CZ']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all of the voices\n",
    "voices = engine.getProperty('voices')\n",
    "\n",
    "# Loop over voices and print their descriptions\n",
    "for index, voice in enumerate(voices):\n",
    "    print(\"Voice {}: \".format(index))\n",
    "    print(\" - ID: %s\" % voice.id)\n",
    "    print(\" - Name: %s\" % voice.name)\n",
    "    print(\" - Languages: %s\" % voice.languages)\n",
    "    print(\" - Gender: %s\" % voice.gender)\n",
    "    print(\" - Age: %s\" % voice.age)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5UgYSzWX4LSS"
   },
   "outputs": [],
   "source": [
    "### Voice properties    \n",
    "\n",
    "# Speed percent (can go over 100)\n",
    "engine.setProperty(name = 'rate', value = 180)    \n",
    "\n",
    "# Volume 0-1\n",
    "engine.setProperty(name = 'volume', value = 0.9)\n",
    "\n",
    "# Voice ID\n",
    "en_voice_id = \"com.apple.speech.synthesis.voice.daniel.premium\"\n",
    "engine.setProperty('voice', en_voice_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PeiVQ4Fk4LSW"
   },
   "outputs": [],
   "source": [
    "# Convert the text to speech\n",
    "engine.say(\"You've got mail!\")\n",
    "engine.say(\"The pyttsx3 module supports native Windows and Mac speech APIs but also supports espeak, making it the best available text-to-speech package.\")\n",
    "engine.runAndWait() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K0Fu37lE4LSc"
   },
   "source": [
    "<br>\n",
    "\n",
    "# 5. Finalize your Conversational Based Agent\n",
    "\n",
    "---\n",
    "\n",
    "Now it's time to put everything together so you can do speech-to-text, text-to-text, and text-to-speech at the same time. For this, you will create a button which after pushing you can speak and your model will speck to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w4jgCKIc4LSd"
   },
   "outputs": [],
   "source": [
    "# Import the libraries \n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from text_to_text import text_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversational based agent \n",
    "def agent():\n",
    "    speak_button = widgets.Button(description=\"Click and speak!\")\n",
    "    speak_output = widgets.Output()\n",
    "    display(speak_button, speak_output)\n",
    "    \n",
    "    def on_button_clicked(b):\n",
    "        with speak_output:\n",
    "            # Speech recognition\n",
    "            print(\"Say Something...\")\n",
    "            text = streaming_recognition()\n",
    "            # Text-to-text\n",
    "            response = text_to_text(text, enc_model, dec_model, str_to_tokens, preprocess_text, tokenizer, maxlen_answers)\n",
    "            print(\" + AGENT: \", response)\n",
    "            # Text to speech\n",
    "            engine.say(response)\n",
    "            engine.runAndWait() \n",
    "            print(\"\")\n",
    "\n",
    "    \n",
    "    speak_button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d472b3af0d04c43b2626b42a911e685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Click and speak!', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f3d76dde2f44588d883f8d86b6e9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Talk to your agent\n",
    "agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YrdHkW7P4LSt"
   },
   "source": [
    "# Good Job!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Conversational Based Agent.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "ama",
   "language": "python",
   "name": "ama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
