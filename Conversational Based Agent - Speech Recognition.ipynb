{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKV2BEL76euo"
   },
   "source": [
    "<h1 style=\"text-align: center;text-transform: uppercase;\">Conversational Based Agent</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "In this project, you will build an end-to-end voice conversational agent, which can take a voice input audio line, and synthesize a response. The chatbot agent will be executed locally on your computer. \n",
    "\n",
    "<img style=\"width:550px; height:300px;\" src=\"assets/intro.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9fuw6EH4LOU"
   },
   "source": [
    "This project consists of the following parts:\n",
    "1. __Speech Recognition:__ <br>In this part, you will create a speech recognition that can convert your voice into a text format.<br><br>\n",
    "2. __Chatbot:__ <br>This is the core of your conversational based agent. You will build a chatbot that will answer your questions. <br><br>\n",
    "3. __Text to Speech:__ <br>After getting the answer from your chatbot, it should be converted into a voice format and that is what you should create in this part. <br><br>\n",
    "4. __Finalize your Conversational Based Agent:__ <br>At the very end step, you will put everything together and create your Conversational Based Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gx13lGaR6evT"
   },
   "source": [
    "<br>\n",
    "\n",
    "# 1. Speech Recognition\n",
    "\n",
    "---\n",
    "\n",
    "We will use the Mozilla <a href=\"https://github.com/mozilla/DeepSpeech\">DeepSpeech</a> open-sourced implementation originally developed by Baidu. This allows speech recognition directly on your computer instead of requiring an internet connection or setting up a cloud account.\n",
    "\n",
    "While DeepSpeech is not the state-of-the-art speech recognizer (there is now DeepSpeech2, Wave2Letter by Facebook, and \n",
    "the RNN Transducer by Google), DeepSpeech is a fast, lightweight implementation which is suitable for real-time transcription with very high accuracy. Its code is also well-maintained with new features being added regularly.\n",
    "\n",
    "\n",
    "In this project, we will not train our own speech recognition model (a fairly challenging project), but will use an open-sourced pre-trained model.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepspeech\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "from scipy.io.wavfile import write\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import queue\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to download the DeepSpeech model, with the matching version (0.7.4) that we installed.\n",
    "There two files that are required. Please download and save them under the `speech_recognizer` folder.\n",
    "\n",
    "1. https://github.com/mozilla/DeepSpeech/releases/download/v0.7.4/deepspeech-0.7.4-models.pbmm\n",
    "2. https://github.com/mozilla/DeepSpeech/releases/download/v0.7.4/deepspeech-0.7.4-models.scorer\n",
    "\n",
    "Once you have these two files, we are ready to perform speech recognition by instantiating the DeepSpeech Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = deepspeech.Model('speech_recognizer/deepspeech-0.7.4-models.pbmm')\n",
    "ds.enableExternalScorer('speech_recognizer/deepspeech-0.7.4-models.scorer')\n",
    "_ = ds.setScorerAlphaBeta(0.75, 1.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Speech-recognition on single audio file\n",
    "\n",
    "In this section, let's set up the basic functionality of running speech recognition on a single audio file. \n",
    "\n",
    "1. recording a .wav audio file with a fix d length (say 3 seconds)\n",
    "\n",
    "2. perform speech recognition from the saved .wav file using the DeepSpeech model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name = 'audio_files/test_audio.wav'\n",
    "sample_rate = 16000\n",
    "seconds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording Finished!\n"
     ]
    }
   ],
   "source": [
    "sleep(0.5)\n",
    "print(\"Recording...\")\n",
    "audio_array = sd.rec(int(seconds * sample_rate), samplerate = sample_rate, channels = 1)\n",
    "\n",
    "# Wait until recording is finished\n",
    "sd.wait() \n",
    "\n",
    "# Finished recording print\n",
    "print(\"Recording Finished!\")\n",
    "\n",
    "# Save as WAV file \n",
    "write(test_file_name, sample_rate, audio_array) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sd.rec` function gives us numpy array directly! We can check its shape\n",
    "\n",
    "The number of rows is seconds * sample_rate = 16000 * 4, the number of columns is the channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 1)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the recording by playing it back from the numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0721478e-02],\n",
       "       [ 8.8054694e-02],\n",
       "       [-2.7549866e-01],\n",
       "       ...,\n",
       "       [ 8.7510690e-04],\n",
       "       [ 1.4510446e-41],\n",
       "       [ 1.7442986e+28]], dtype=float32)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd.playrec(audio_array, sample_rate, channels=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or from the .wav file that it is saved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, fs = sf.read(test_file_name, dtype='float32')\n",
    "sd.play(data, sample_rate, device=1)\n",
    "status = sd.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the playback did not work, chooose another output device by checking what is available on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "> 0 Built-in Microphone, Core Audio (2 in, 0 out)\n",
       "< 1 Built-in Output, Core Audio (0 in, 2 out)\n",
       "  2 USB PnP Audio Device, Core Audio (0 in, 2 out)\n",
       "  3 USB PnP Audio Device, Core Audio (1 in, 0 out)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd.query_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While sound device outputs numpy array in float32 datatype (from -1 to 1), DeepSpeech speech recognizer expects a 16bit int type (-32768 to 32767). Let's convert the numpy array and set the correct data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_array *= 32768\n",
    "audio_array = audio_array.astype('int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test recording'"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.stt(audio_array[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Streaming Speech Recognition in Real-Time\n",
    "\n",
    "Recording your voice then running speech recognition on a audio file works fine, but it is not very user friendly. The interaction is slow and not easy to use in a continuous setting.\n",
    "\n",
    "In this section, let's setup a function to recording your voice AND recognize the text at the same time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(indata, frames, time, status):\n",
    "    \"\"\"This is called (from a separate thread) for each audio block.\"\"\"\n",
    "    if status:\n",
    "        print(status, file=sys.stderr)\n",
    "    q.put(indata.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "press Ctrl+C to stop the recording\n",
      "################################################################################\n",
      "\n",
      "the moon is about two hundred and fifty thousand miles from earth on average\n"
     ]
    }
   ],
   "source": [
    "q = queue.Queue()\n",
    "recognizer_stream = ds.createStream()\n",
    "try:\n",
    "    with sd.InputStream(samplerate=sample_rate, device=0, channels=2, callback=callback) as audio_stream:\n",
    "        print('#' * 80)\n",
    "        print('press Interrupt to stop the recording')\n",
    "        print('#' * 80)\n",
    "        print()\n",
    "        i = 0\n",
    "        while True:\n",
    "            i += 1\n",
    "            audio_chunk = q.get()\n",
    "            audio_chunk *= 32768\n",
    "            audio_chunk = audio_chunk.astype('int16')\n",
    "            recognizer_stream.feedAudioContent(audio_chunk[:,0])\n",
    "            text = recognizer_stream.intermediateDecode()\n",
    "            print(f'\\r{text}', end='')\n",
    "except KeyboardInterrupt:\n",
    "#     print('\\r\\nRecording finished.\\r\\n')\n",
    "    pass\n",
    "finally:\n",
    "    audio_stream.stop()\n",
    "    audio_stream.close()\n",
    "    audio_chunks = []\n",
    "    while True:\n",
    "        if not q.empty():\n",
    "            chunk = q.get()\n",
    "            audio_chunks.append(chunk)\n",
    "        else:\n",
    "            break\n",
    "    if audio_chunks:\n",
    "        audio_chunks = np.concatenate(audio_chunks)\n",
    "        audio_chunk *= 32768\n",
    "        audio_chunk = audio_chunk.astype('int16')\n",
    "        recognizer_stream.feedAudioContent(audio_chunk[:,0])\n",
    "    text = recognizer_stream.finishStream()\n",
    "    print(f'\\r{text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_recognition():\n",
    "    q = queue.Queue()\n",
    "    recognizer_stream = ds.createStream()\n",
    "    \n",
    "    def callback(indata, frames, time, status):\n",
    "        \"\"\"This is called (from a separate thread) for each audio block.\"\"\"\n",
    "        if status:\n",
    "            print(status, file=sys.stderr)\n",
    "        q.put(indata.copy())\n",
    "    \n",
    "    try:\n",
    "        with sd.InputStream(samplerate=sample_rate, device=0, channels=2, callback=callback) as audio_stream:\n",
    "            while True:\n",
    "                audio_chunk = q.get()\n",
    "                audio_chunk *= 32768\n",
    "                audio_chunk = audio_chunk.astype('int16')\n",
    "                recognizer_stream.feedAudioContent(audio_chunk[:,0])\n",
    "                text = recognizer_stream.intermediateDecode()\n",
    "                print(f\"\\r - YOU SAID: {text}\", end='')\n",
    "    except KeyboardInterrupt:\n",
    "    #     print('\\r\\nRecording finished.\\r\\n')\n",
    "        pass\n",
    "    finally:\n",
    "        audio_stream.stop()\n",
    "        audio_stream.close()\n",
    "        audio_chunks = []\n",
    "        while True:\n",
    "            if not q.empty():\n",
    "                chunk = q.get()\n",
    "                audio_chunks.append(chunk)\n",
    "            else:\n",
    "                break\n",
    "        if audio_chunks:\n",
    "            audio_chunks = np.concatenate(audio_chunks)\n",
    "            audio_chunk *= 32768\n",
    "            audio_chunk = audio_chunk.astype('int16')\n",
    "            recognizer_stream.feedAudioContent(audio_chunk[:,0])\n",
    "        text = recognizer_stream.finishStream()\n",
    "        print(f\"\\r - YOU SAID: {text}\", end='\\r\\n')\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "press Interrupt to stop the recording\n",
      "################################################################################\n",
      "\n",
      "this is a test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'this is a test'"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming_recognition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congratulations! You are now able to run your own speech-to-text!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Conversational Based Agent.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "ama",
   "language": "python",
   "name": "ama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
