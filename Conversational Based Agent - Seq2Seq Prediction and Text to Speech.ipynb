{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKV2BEL76euo"
   },
   "source": [
    "<h1 style=\"text-align: center;text-transform: uppercase;\">Conversational Based Agent</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "In this project, you will build an end-to-end voice conversational agent, which can take a voice input audio line, and synthesize a response. The chatbot agent will be executed locally on your computer. \n",
    "\n",
    "<img style=\"width:550px; height:300px;\" src=\"assets/intro.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9fuw6EH4LOU"
   },
   "source": [
    "This project consists of the following parts:\n",
    "1. __Speech Recognition:__ <br>In this part, you will create a speech recognition that can convert your voice into a text format.<br><br>\n",
    "2. __Chatbot:__ <br>This is the core of your conversational based agent. You will build a chatbot that will answer your questions. <br><br>\n",
    "3. __Text to Speech:__ <br>After getting the answer from your chatbot, it should be converted into a voice format and that is what you should create in this part. <br><br>\n",
    "4. __Finalize your Conversational Based Agent:__ <br>At the very end step, you will put everything together and create your Conversational Based Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tIqinjUX6ewC"
   },
   "source": [
    "<br>\n",
    "\n",
    "# 2. Chatbot\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Now that you have a Seq2Seq model trained, it is time to implement the prediction module.\n",
    "\n",
    "The prediction for Seq2Seq model is more complicated than a typical machine learning model, because it involves the transfer of encoder and decoder states, as well as the sequential nature of the inputs.\n",
    "\n",
    "Let's work through this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w521SVsleJ0F"
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eq9ZLFSEKmNi"
   },
   "source": [
    "Below you can play around with hyperparameters for improving the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RL28XUCmLb93"
   },
   "source": [
    "First, we will need to recreate the Model graph layers that we used for training, load in the weights that were trained, then we will `rearrange` some of the layers to produce the prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlV38YEgeJ0Q"
   },
   "outputs": [],
   "source": [
    "### Encoder Input\n",
    "embed_dim = 200\n",
    "num_lstm = 200\n",
    "\n",
    "# Input for encoder\n",
    "encoder_inputs = Input(shape = (None, ), name='encoder_inputs')\n",
    "\n",
    "# Embedding layer\n",
    "# Why mask_zero = True? https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "encoder_embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = embed_dim, mask_zero = True, name='encoder_embedding')(encoder_inputs)\n",
    "\n",
    "# LSTM layer (that returns states in addition to output)\n",
    "encoder_outputs, state_h, state_c = LSTM(units = num_lstm, return_state = True, name='encoder_lstm')(encoder_embedding)\n",
    "\n",
    "# Get the states for encoder\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "### Decoder\n",
    "\n",
    "# Input for decoder\n",
    "decoder_inputs = Input(shape = (None,  ), name='decoder_inputs')\n",
    "\n",
    "# Embedding layer\n",
    "decoder_embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = embed_dim , mask_zero = True, name='decoder_embedding')(decoder_inputs)\n",
    "\n",
    "# LSTM layer (that returns states and sequences as well)\n",
    "decoder_lstm = LSTM(units = num_lstm , return_state = True , return_sequences = True, name='decoder_lstm')\n",
    "\n",
    "# Get the output of LSTM layer, using the initial states from the encoder\n",
    "decoder_outputs, _, _ = decoder_lstm(inputs = decoder_embedding, initial_state = encoder_states)\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense = Dense(units = VOCAB_SIZE, activation = softmax, name='output') \n",
    "\n",
    "# Get the output of Dense layer\n",
    "output = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Create the model\n",
    "model = Model([encoder_inputs, decoder_inputs], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1575581175908,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "CKrCrmA26eyQ",
    "outputId": "49f944e7-a3ae-4e33-9622-755d28fd8b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, None, 200)    387800      encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, None, 200)    387800      decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, 200), (None, 320800      encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 200),  320800      decoder_embedding[0][0]          \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, None, 1939)   389739      decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,806,939\n",
      "Trainable params: 1,806,939\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1385,
     "status": "ok",
     "timestamp": 1575581737849,
     "user": {
      "displayName": "soheil mohammadpour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADGOX2Mrqnd4Kv8uSoUa349iTKp3mAzZjVlkqF=s64",
      "userId": "06946141564410396693"
     },
     "user_tz": -240
    },
    "id": "tkVPvGd74LRK",
    "outputId": "5a3a9d6e-4684-417d-f7ae-8824b57f0a8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Weight Loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load the final model\n",
    "model.load_weights(f'saved_models/final_weight_2020-08-01-16-28-47.h5') \n",
    "print(\"Model Weight Loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F33sverZeJ0b"
   },
   "source": [
    "<br>\n",
    "\n",
    "### 3.6. Inference\n",
    "\n",
    "---\n",
    "\n",
    "Now it's time to use our model for inference. In other words, we will ask a question to our chatbot and it will answer us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GaDAPDOa6eyb"
   },
   "outputs": [],
   "source": [
    "# Function for making inference\n",
    "def make_inference_models():\n",
    "    \n",
    "    # Create a model that takes encoder's input and outputs the states for encoder\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    # Create two inputs for decoder which are hidden state (or state h) and cell state (or state c)\n",
    "    decoder_state_input_h = Input(shape = (num_lstm, ))\n",
    "    decoder_state_input_c = Input(shape = (num_lstm, ))\n",
    "    \n",
    "    # Store the two inputs for decoder inside a list\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    # Pass the inputs through LSTM layer you have created before\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state = decoder_states_inputs)\n",
    "    \n",
    "    # Store the outputted hidden state and cell state from LSTM inside a list\n",
    "    decoder_states = [state_h, state_c]\n",
    "\n",
    "    # Pass the output from LSTM layer through the dense layer you have created before\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Create a model that takes decoder_inputs and decoder_states_inputs as inputs and outputs decoder_outputs and decoder_states\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                          [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NWYFiofO6eye"
   },
   "outputs": [],
   "source": [
    "# Function for converting strings to tokens\n",
    "def str_to_tokens(sentence: str, maxlen_questions=22):\n",
    "\n",
    "    # Lowercase the sentence and split it into words\n",
    "    words = sentence.lower().split()\n",
    "\n",
    "    # Initialize a list for tokens\n",
    "    tokens_list = list()\n",
    "\n",
    "    # Iterate through words\n",
    "    for word in words:\n",
    "\n",
    "        # Append the word index inside tokens list\n",
    "        tokens_list.append(tokenizer.word_index[word]) \n",
    "\n",
    "    # Pad the sequences to be the same length\n",
    "    return pad_sequences([tokens_list] , maxlen = maxlen_questions, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model for inference\n",
    "enc_model , dec_model = make_inference_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer that needs to be used in conjunction with the sequence modelso we can use it elsewhere\n",
    "with open(f'saved_models/tokenizer_{timestamp}.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending conversational agent\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the number of times you want to ask question\n",
    "try:\n",
    "    for _ in range(5):\n",
    "\n",
    "        # Get the input and predict it with the encoder model\n",
    "        states_values = enc_model.predict(str_to_tokens(preprocess_text(input('Enter question : '))))\n",
    "\n",
    "        # Initialize the target sequence with zero - array([[0.]])\n",
    "        empty_target_seq = np.zeros(shape = (1, 1))\n",
    "\n",
    "        # Update the target sequence with index of \"start\"\n",
    "        empty_target_seq[0, 0] = tokenizer.word_index[\"starttoken\"]\n",
    "\n",
    "        # Initialize the stop condition with False\n",
    "        stop_condition = False\n",
    "\n",
    "        # Initialize the decoded words with an empty string\n",
    "        decoded_translation = []\n",
    "\n",
    "        # While stop_condition is false\n",
    "        while not stop_condition :\n",
    "\n",
    "            # Predict the (target sequence + the output from encoder model) with decoder model\n",
    "            dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values)\n",
    "\n",
    "            # Get the index for sampled word\n",
    "            sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
    "\n",
    "            # Initialize the sampled word with None\n",
    "            sampled_word = None\n",
    "\n",
    "            # Iterate through words and their indexes\n",
    "            for word, index in tokenizer.word_index.items() :\n",
    "\n",
    "                # If the index is equal to sampled word's index\n",
    "                if sampled_word_index == index :\n",
    "\n",
    "                    # Add the word to the decoded string\n",
    "                    decoded_translation.append(word)\n",
    "\n",
    "                    # Update the sampled word\n",
    "                    sampled_word = word\n",
    "\n",
    "            # If sampled word is equal to \"end\" OR the length of decoded string is more that what is allowed\n",
    "            if sampled_word == 'endtoken' or len(decoded_translation) > maxlen_answers:\n",
    "\n",
    "                # Make the stop_condition to true\n",
    "                stop_condition = True\n",
    "\n",
    "            # Initialize back the target sequence to zero - array([[0.]])    \n",
    "            empty_target_seq = np.zeros(shape = (1, 1))  \n",
    "\n",
    "            # Update the target sequence with index of \"start\"\n",
    "            empty_target_seq[0, 0] = sampled_word_index\n",
    "\n",
    "            # Get the state values\n",
    "            states_values = [h, c] \n",
    "\n",
    "            # Print the decoded string\n",
    "        print(' '.join(decoded_translation[:-1]))\n",
    "except KeyboardInterrupt:\n",
    "    print('Ending conversational agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k-z2MXSd6ezL"
   },
   "source": [
    "<br>\n",
    "\n",
    "# 4. Text to Speech\n",
    "\n",
    "---\n",
    "\n",
    "In this section, we will use a library called pyttsx3 which performs text-to-speech conversion. Unlike alternative libraries, this works offline and is compatible with both Python 2 and 3.\n",
    "\n",
    "State-of-the-art text-to-voice systems are more difficult to setup and use (such as the Tacotron 2 + WaveNet), and is outside the scope of this course. Interested students can look them up!\n",
    "\n",
    "<a href=\"https://ai.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html\">Tacotron 2</a> <br>\n",
    "https://arxiv.org/abs/1712.05884\n",
    "\n",
    "Checkout some demos using WaveNet.\n",
    "https://cloud.google.com/text-to-speech\n",
    "\n",
    "For a _relatively simple_ implementation of a high quality TTS system that you can run, checkout the Mozilla <a href=\"https://github.com/mozilla/TTS\">TTS</a> project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UOU6Y0DK6ezM"
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQ-pfu4t4LR7"
   },
   "outputs": [],
   "source": [
    "# Construct a new TTS engine instance\n",
    "engine = pyttsx3.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVD7Nu4o4LSE",
    "outputId": "fa4f114c-ea72-45a8-a254-6d9e39bc9fb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voice 0: \n",
      " - ID: com.apple.speech.synthesis.voice.Alex\n",
      " - Name: Alex\n",
      " - Languages: ['en_US']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 1: \n",
      " - ID: com.apple.speech.synthesis.voice.alice\n",
      " - Name: Alice\n",
      " - Languages: ['it_IT']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 2: \n",
      " - ID: com.apple.speech.synthesis.voice.alva\n",
      " - Name: Alva\n",
      " - Languages: ['sv_SE']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 3: \n",
      " - ID: com.apple.speech.synthesis.voice.amelie\n",
      " - Name: Amelie\n",
      " - Languages: ['fr_CA']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 4: \n",
      " - ID: com.apple.speech.synthesis.voice.anna\n",
      " - Name: Anna\n",
      " - Languages: ['de_DE']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 5: \n",
      " - ID: com.apple.speech.synthesis.voice.carmit\n",
      " - Name: Carmit\n",
      " - Languages: ['he_IL']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 6: \n",
      " - ID: com.apple.speech.synthesis.voice.damayanti\n",
      " - Name: Damayanti\n",
      " - Languages: ['id_ID']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 7: \n",
      " - ID: com.apple.speech.synthesis.voice.daniel\n",
      " - Name: Daniel\n",
      " - Languages: ['en_GB']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 8: \n",
      " - ID: com.apple.speech.synthesis.voice.diego\n",
      " - Name: Diego\n",
      " - Languages: ['es_AR']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 9: \n",
      " - ID: com.apple.speech.synthesis.voice.ellen\n",
      " - Name: Ellen\n",
      " - Languages: ['nl_BE']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 10: \n",
      " - ID: com.apple.speech.synthesis.voice.fiona\n",
      " - Name: Fiona\n",
      " - Languages: ['en-scotland']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 11: \n",
      " - ID: com.apple.speech.synthesis.voice.Fred\n",
      " - Name: Fred\n",
      " - Languages: ['en_US']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 30\n",
      "\n",
      "Voice 12: \n",
      " - ID: com.apple.speech.synthesis.voice.ioana\n",
      " - Name: Ioana\n",
      " - Languages: ['ro_RO']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 13: \n",
      " - ID: com.apple.speech.synthesis.voice.joana\n",
      " - Name: Joana\n",
      " - Languages: ['pt_PT']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 14: \n",
      " - ID: com.apple.speech.synthesis.voice.jorge\n",
      " - Name: Jorge\n",
      " - Languages: ['es_ES']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 15: \n",
      " - ID: com.apple.speech.synthesis.voice.juan\n",
      " - Name: Juan\n",
      " - Languages: ['es_MX']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 16: \n",
      " - ID: com.apple.speech.synthesis.voice.kanya\n",
      " - Name: Kanya\n",
      " - Languages: ['th_TH']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 17: \n",
      " - ID: com.apple.speech.synthesis.voice.karen\n",
      " - Name: Karen\n",
      " - Languages: ['en_AU']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 18: \n",
      " - ID: com.apple.speech.synthesis.voice.kyoko\n",
      " - Name: Kyoko\n",
      " - Languages: ['ja_JP']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 19: \n",
      " - ID: com.apple.speech.synthesis.voice.laura\n",
      " - Name: Laura\n",
      " - Languages: ['sk_SK']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 20: \n",
      " - ID: com.apple.speech.synthesis.voice.lekha\n",
      " - Name: Lekha\n",
      " - Languages: ['hi_IN']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 21: \n",
      " - ID: com.apple.speech.synthesis.voice.luca\n",
      " - Name: Luca\n",
      " - Languages: ['it_IT']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 22: \n",
      " - ID: com.apple.speech.synthesis.voice.luciana\n",
      " - Name: Luciana\n",
      " - Languages: ['pt_BR']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 23: \n",
      " - ID: com.apple.speech.synthesis.voice.maged\n",
      " - Name: Maged\n",
      " - Languages: ['ar_SA']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 24: \n",
      " - ID: com.apple.speech.synthesis.voice.mariska\n",
      " - Name: Mariska\n",
      " - Languages: ['hu_HU']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 25: \n",
      " - ID: com.apple.speech.synthesis.voice.mei-jia\n",
      " - Name: Mei-Jia\n",
      " - Languages: ['zh_TW']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 26: \n",
      " - ID: com.apple.speech.synthesis.voice.melina\n",
      " - Name: Melina\n",
      " - Languages: ['el_GR']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 27: \n",
      " - ID: com.apple.speech.synthesis.voice.milena\n",
      " - Name: Milena\n",
      " - Languages: ['ru_RU']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 28: \n",
      " - ID: com.apple.speech.synthesis.voice.moira\n",
      " - Name: Moira\n",
      " - Languages: ['en_IE']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 29: \n",
      " - ID: com.apple.speech.synthesis.voice.monica\n",
      " - Name: Monica\n",
      " - Languages: ['es_ES']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 30: \n",
      " - ID: com.apple.speech.synthesis.voice.nora\n",
      " - Name: Nora\n",
      " - Languages: ['nb_NO']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 31: \n",
      " - ID: com.apple.speech.synthesis.voice.paulina\n",
      " - Name: Paulina\n",
      " - Languages: ['es_MX']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 32: \n",
      " - ID: com.apple.speech.synthesis.voice.samantha\n",
      " - Name: Samantha\n",
      " - Languages: ['en_US']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 33: \n",
      " - ID: com.apple.speech.synthesis.voice.sara\n",
      " - Name: Sara\n",
      " - Languages: ['da_DK']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 34: \n",
      " - ID: com.apple.speech.synthesis.voice.satu\n",
      " - Name: Satu\n",
      " - Languages: ['fi_FI']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 35: \n",
      " - ID: com.apple.speech.synthesis.voice.sin-ji\n",
      " - Name: Sin-ji\n",
      " - Languages: ['zh_HK']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 36: \n",
      " - ID: com.apple.speech.synthesis.voice.tessa\n",
      " - Name: Tessa\n",
      " - Languages: ['en_ZA']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 37: \n",
      " - ID: com.apple.speech.synthesis.voice.thomas\n",
      " - Name: Thomas\n",
      " - Languages: ['fr_FR']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 38: \n",
      " - ID: com.apple.speech.synthesis.voice.ting-ting\n",
      " - Name: Ting-Ting\n",
      " - Languages: ['zh_CN']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 39: \n",
      " - ID: com.apple.speech.synthesis.voice.veena\n",
      " - Name: Veena\n",
      " - Languages: ['en_IN']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 40: \n",
      " - ID: com.apple.speech.synthesis.voice.Victoria\n",
      " - Name: Victoria\n",
      " - Languages: ['en_US']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 41: \n",
      " - ID: com.apple.speech.synthesis.voice.xander\n",
      " - Name: Xander\n",
      " - Languages: ['nl_NL']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 42: \n",
      " - ID: com.apple.speech.synthesis.voice.yelda\n",
      " - Name: Yelda\n",
      " - Languages: ['tr_TR']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 43: \n",
      " - ID: com.apple.speech.synthesis.voice.yuna\n",
      " - Name: Yuna\n",
      " - Languages: ['ko_KR']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 44: \n",
      " - ID: com.apple.speech.synthesis.voice.yuri\n",
      " - Name: Yuri\n",
      " - Languages: ['ru_RU']\n",
      " - Gender: VoiceGenderMale\n",
      " - Age: 35\n",
      "\n",
      "Voice 45: \n",
      " - ID: com.apple.speech.synthesis.voice.zosia\n",
      " - Name: Zosia\n",
      " - Languages: ['pl_PL']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n",
      "Voice 46: \n",
      " - ID: com.apple.speech.synthesis.voice.zuzana\n",
      " - Name: Zuzana\n",
      " - Languages: ['cs_CZ']\n",
      " - Gender: VoiceGenderFemale\n",
      " - Age: 35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all of the voices\n",
    "voices = engine.getProperty('voices')\n",
    "\n",
    "# Loop over voices and print their descriptions\n",
    "for index, voice in enumerate(voices):\n",
    "    print(\"Voice {}: \".format(index))\n",
    "    print(\" - ID: %s\" % voice.id)\n",
    "    print(\" - Name: %s\" % voice.name)\n",
    "    print(\" - Languages: %s\" % voice.languages)\n",
    "    print(\" - Gender: %s\" % voice.gender)\n",
    "    print(\" - Age: %s\" % voice.age)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5UgYSzWX4LSS"
   },
   "outputs": [],
   "source": [
    "### Voice properties    \n",
    "\n",
    "# Speed percent (can go over 100)\n",
    "engine.setProperty(name = 'rate', value = 180)    \n",
    "\n",
    "# Volume 0-1\n",
    "engine.setProperty(name = 'volume', value = 0.9)\n",
    "\n",
    "# Voice ID\n",
    "en_voice_id = \"com.apple.speech.synthesis.voice.daniel.premium\"\n",
    "engine.setProperty('voice', en_voice_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PeiVQ4Fk4LSW"
   },
   "outputs": [],
   "source": [
    "# Convert the text to speech\n",
    "engine.say(\"You've got mail!\")\n",
    "engine.say(\"The pyttsx3 module supports native Windows and Mac speech APIs but also supports espeak, making it the best available text-to-speech package.\")\n",
    "engine.runAndWait() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K0Fu37lE4LSc"
   },
   "source": [
    "<br>\n",
    "\n",
    "# 5. Finalize your Conversational Based Agent\n",
    "\n",
    "---\n",
    "\n",
    "Now it's time to put everything together so you can do speech-to-text, text-to-text, and text-to-speech at the same time. For this, you will create a button which after pushing you can speak and your model will speck to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w4jgCKIc4LSd"
   },
   "outputs": [],
   "source": [
    "# Import the libraries \n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from text_to_text import text_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversational based agent \n",
    "def agent():\n",
    "    speak_button = widgets.Button(description=\"Click and speak!\")\n",
    "    speak_output = widgets.Output()\n",
    "    display(speak_button, speak_output)\n",
    "    \n",
    "    def on_button_clicked(b):\n",
    "        with speak_output:\n",
    "            # Speech recognition\n",
    "            print(\"Say Something...\")\n",
    "            text = streaming_recognition()\n",
    "            # Text-to-text\n",
    "            response = text_to_text(text, enc_model, dec_model, str_to_tokens, preprocess_text, tokenizer, maxlen_answers)\n",
    "            print(\" + AGENT: \", response)\n",
    "            # Text to speech\n",
    "            engine.say(response)\n",
    "            engine.runAndWait() \n",
    "            print(\"\")\n",
    "\n",
    "    \n",
    "    speak_button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d472b3af0d04c43b2626b42a911e685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Click and speak!', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f3d76dde2f44588d883f8d86b6e9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Talk to your agent\n",
    "agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YrdHkW7P4LSt"
   },
   "source": [
    "# Good Job!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Conversational Based Agent.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "ama",
   "language": "python",
   "name": "ama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
